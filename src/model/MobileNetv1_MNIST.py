import torch.nn as nn

class MobileNetv1_MNIST(nn.Module):
    def __init__(self):
        super(MobileNetv1_MNIST, self).__init__()
        self.layer1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)
        self.layer2 = nn.BatchNorm2d(32)
        self.layer3 = nn.ReLU()
        self.layer4 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1)
        self.layer5 = nn.BatchNorm2d(32)
        self.layer6 = nn.ReLU()
        self.layer7 = nn.Conv2d(32, 64, kernel_size=1, stride=1)
        self.layer8 = nn.BatchNorm2d(64)
        self.layer9 = nn.ReLU()
        self.layer10 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1)
        self.layer11 = nn.BatchNorm2d(64)
        self.layer12 = nn.ReLU()
        self.layer13 = nn.Conv2d(64, 128, kernel_size=1, stride=1)
        self.layer14 = nn.BatchNorm2d(128)
        self.layer15 = nn.ReLU()
        self.layer16 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)
        self.layer17 = nn.BatchNorm2d(128)
        self.layer18 = nn.ReLU()
        self.layer19 = nn.Conv2d(128, 128, kernel_size=1, stride=1)
        self.layer20 = nn.BatchNorm2d(128)
        self.layer21 = nn.ReLU()
        self.layer22 = nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1)
        self.layer23 = nn.BatchNorm2d(128)
        self.layer24 = nn.ReLU()
        self.layer25 = nn.Conv2d(128, 256, kernel_size=1, stride=1)
        self.layer26 = nn.BatchNorm2d(256)
        self.layer27 = nn.ReLU()
        self.layer28 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)
        self.layer29 = nn.BatchNorm2d(256)
        self.layer30 = nn.ReLU()
        self.layer31 = nn.Conv2d(256, 256, kernel_size=1, stride=1)
        self.layer32 = nn.BatchNorm2d(256)
        self.layer33 = nn.ReLU()
        self.layer34 = nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1)
        self.layer35 = nn.BatchNorm2d(256)
        self.layer36 = nn.ReLU()
        self.layer37 = nn.Conv2d(256, 512, kernel_size=1, stride=1)
        self.layer38 = nn.BatchNorm2d(512)
        self.layer39 = nn.ReLU()
        self.layer40 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)
        self.layer41 = nn.BatchNorm2d(512)
        self.layer42 = nn.ReLU()
        self.layer43 = nn.Conv2d(512, 512, kernel_size=1, stride=1)
        self.layer44 = nn.BatchNorm2d(512)
        self.layer45 = nn.ReLU()
        self.layer46 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)
        self.layer47 = nn.BatchNorm2d(512)
        self.layer48 = nn.ReLU()
        self.layer49 = nn.Conv2d(512, 512, kernel_size=1, stride=1)
        self.layer50 = nn.BatchNorm2d(512)
        self.layer51 = nn.ReLU()
        self.layer52 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)
        self.layer53 = nn.BatchNorm2d(512)
        self.layer54 = nn.ReLU()
        self.layer55 = nn.Conv2d(512, 512, kernel_size=1, stride=1)
        self.layer56 = nn.BatchNorm2d(512)
        self.layer57 = nn.ReLU()
        self.layer58 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)
        self.layer59 = nn.BatchNorm2d(512)
        self.layer60 = nn.ReLU()
        self.layer61 = nn.Conv2d(512, 512, kernel_size=1, stride=1)
        self.layer62 = nn.BatchNorm2d(512)
        self.layer63 = nn.ReLU()
        self.layer64 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)
        self.layer65 = nn.BatchNorm2d(512)
        self.layer66 = nn.ReLU()
        self.layer67 = nn.Conv2d(512, 512, kernel_size=1, stride=1)
        self.layer68 = nn.BatchNorm2d(512)
        self.layer69 = nn.ReLU()
        self.layer70 = nn.Conv2d(512, 512, kernel_size=3, stride=2, padding=1)
        self.layer71 = nn.BatchNorm2d(512)
        self.layer72 = nn.ReLU()
        self.layer73 = nn.Conv2d(512, 1024, kernel_size=1, stride=1)
        self.layer74 = nn.BatchNorm2d(1024)
        self.layer75 = nn.ReLU()
        self.layer76 = nn.Conv2d(1024, 1024, kernel_size=3, stride=1, padding=1)
        self.layer77 = nn.BatchNorm2d(1024)
        self.layer78 = nn.ReLU()
        self.layer79 = nn.Conv2d(1024, 1024, kernel_size=1, stride=1)
        self.layer80 = nn.BatchNorm2d(1024)
        self.layer81 = nn.ReLU()
        self.layer82 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.layer83 = nn.Flatten(1, -1)
        self.layer84 = nn.Linear(1024, 10)

    def forward(self, x):
        for i in range(1, 85):
            x = getattr(self, f'layer{i}')(x)
        return x
